{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XofGrNTpy4jB"
      },
      "source": [
        "\"We certify that the code and data in this assignment were generated\n",
        "independently, using only the tools and resources defined in the course\n",
        "and that I/we did not receive any external help, coaching or contributions\n",
        "during the production of this work.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "680cTIufbeJJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9k9Pl7Kiw1k"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1C8IHOQiw1m"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class LudoEnv():\n",
        "    \n",
        "    def __init__(self, BLOCK_LEN=13, NUM_PLAYERS = 2, NUM_PIECES = 4):\n",
        "        self.BLOCK_LEN = BLOCK_LEN     # 13\n",
        "        self.NUM_PLAYERS = NUM_PLAYERS    # 4\n",
        "        self.MAX_LEN = self.BLOCK_LEN * self.NUM_PLAYERS\n",
        "        \n",
        "        self.NUM_PIECES = NUM_PIECES\n",
        "        self.MAX_MOVES = 2\n",
        "        \n",
        "        # add kill check if open move doesn't match first safe\n",
        "        self.SAFE_SPOTS = [0, 3] # to be less than block len\n",
        "        self.ALL_SAFE_SPOTS = []\n",
        "        \n",
        "        for i in range(self.NUM_PLAYERS):\n",
        "            for j in self.SAFE_SPOTS:\n",
        "                self.ALL_SAFE_SPOTS.append(j + self.BLOCK_LEN*i)\n",
        "        \n",
        "        self.DICE_SIDES = 3      # 6\n",
        "        self.BONUS_NUM = self.DICE_SIDES\n",
        "        self.ILLEGAL_AVOIDANCE = 10\n",
        "        self.verbose = False\n",
        "        \n",
        "        self.RE_ORIENT = True        # to reduce total number of states by avoiding meaningfully same states\n",
        "    \n",
        "    def reset(self):\n",
        "\n",
        "        self.home_space = np.array([self.NUM_PIECES]*self.NUM_PLAYERS)\n",
        "        self.target_space = np.array([0]*self.NUM_PLAYERS)\n",
        "        \n",
        "        self.moves_remaining = 1\n",
        "        self.moves_made = 0\n",
        "        self.player_turn = 0\n",
        "        self.dice_face = np.random.randint(self.DICE_SIDES) + 1\n",
        "        self.piece_death_occured = False\n",
        "        \n",
        "        self.states = [\n",
        "            [0]*self.NUM_PLAYERS*self.NUM_PIECES,            # default positions\n",
        "            self.dice_face,\n",
        "            self.player_turn,\n",
        "            self.moves_remaining]\n",
        "        \n",
        "        observation = self.states\n",
        "        return observation\n",
        "        \n",
        "    def check_move(self,arg_pos,steps):\n",
        "        if (self.states[0][arg_pos] == -1) :\n",
        "            if (steps == self.BONUS_NUM):\n",
        "                return 0                          # opening move\n",
        "            else:\n",
        "                return -1                         # illegal move\n",
        "        elif (self.states[0][arg_pos] + steps < self.MAX_LEN):\n",
        "            return 1                              # normal move\n",
        "        elif (self.states[0][arg_pos] + steps == self.MAX_LEN):\n",
        "            return 2                              # target reached\n",
        "        return -1                                 # illegal move\n",
        "    \n",
        "    def move(self, player_num, piece_num,steps):\n",
        "        \n",
        "        self.piece_death_occured = False\n",
        "        if steps == self.BONUS_NUM:\n",
        "            self.moves_remaining += 1\n",
        "        \n",
        "        reward = 0\n",
        "        arg_pos = player_num*self.NUM_PIECES + piece_num\n",
        "        move_opt = self.check_move(arg_pos,steps)\n",
        "        \n",
        "        if move_opt == -1:\n",
        "            for attempt_no in range(self.ILLEGAL_AVOIDANCE):\n",
        "                piece_num = np.random.randint(self.NUM_PIECES)\n",
        "                arg_pos = player_num*self.NUM_PIECES + piece_num\n",
        "                move_opt = self.check_move(arg_pos,steps)\n",
        "                if move_opt != -1:\n",
        "                    break\n",
        "                \n",
        "        if move_opt in [1,2]:\n",
        "            self.states[0][arg_pos] += steps\n",
        "            if move_opt == 1:\n",
        "                self.check_kill(self.states[0][arg_pos], player_num)\n",
        "            if move_opt == 2:\n",
        "                self.moves_remaining += 1         # bonus move for reaching target\n",
        "                reward += 10\n",
        "                self.target_space[player_num] += 1\n",
        "                        \n",
        "        if move_opt == 0 :\n",
        "            \n",
        "            self.states[0][arg_pos] += 1          # add kill check if open move doesn't match first safe spot\n",
        "            self.home_space[player_num] -= 1\n",
        "            \n",
        "        self.moves_remaining -= 1\n",
        "        self.moves_made += 1\n",
        "        \n",
        "        if self.RE_ORIENT:\n",
        "            self.states[0][player_num * self.NUM_PIECES:(player_num +1) * self.NUM_PIECES] = sorted(self.states[0][player_num * self.NUM_PIECES:(player_num +1) * self.NUM_PIECES])\n",
        "        \n",
        "        return reward, move_opt\n",
        "        \n",
        "    def check_kill(self , position, player_num):\n",
        "        \n",
        "        piece_death_occured = False\n",
        "        \n",
        "        if position not in self.ALL_SAFE_SPOTS:\n",
        "            \n",
        "            position = (position + self.BLOCK_LEN * player_num )% self.MAX_LEN\n",
        "            \n",
        "            other_players = np.array([i for i  in range(self.NUM_PLAYERS) if i != player_num])\n",
        "            \n",
        "            # shifted positions by BLOCK_LEN\n",
        "            battle_positions = [(i + int(n/self.NUM_PLAYERS)*self.BLOCK_LEN)%(self.MAX_LEN) if (((i > -1) & (i < self.MAX_LEN)) & (i not in self.ALL_SAFE_SPOTS)) else np.NaN for n,i in  enumerate(self.states[0])]\n",
        "            battle_positions_grid = np.reshape(battle_positions, (self.NUM_PLAYERS,self.NUM_PIECES))\n",
        "            \n",
        "            killed_players = other_players[(battle_positions_grid[other_players] == position).any(1)]\n",
        "            killed_pieces = np.arange(self.NUM_PIECES)[(battle_positions_grid[other_players] == position).any(0)]\n",
        "            \n",
        "            for player_killed in killed_players:\n",
        "                for piece_killed in killed_pieces:\n",
        "                    \n",
        "                    arg_pos = player_killed*self.NUM_PIECES + piece_killed\n",
        "                    position_piece = self.states[0][arg_pos]\n",
        "\n",
        "                    self.states[0][arg_pos] = -1\n",
        "                    self.home_space[player_killed] +=1 \n",
        "                    self.piece_death_occured = True\n",
        "                    \n",
        "                if self.RE_ORIENT:\n",
        "                    self.states[0][player_killed * self.NUM_PIECES:(player_killed +1) * self.NUM_PIECES] = sorted(self.states[0][player_killed * self.NUM_PIECES:(player_killed +1) * self.NUM_PIECES])\n",
        "                        \n",
        "        if self.piece_death_occured:            \n",
        "            self.moves_remaining += 1        # bonus move for getting a kill\n",
        "            if self.verbose:\n",
        "                print('BOOM')\n",
        "\n",
        "            \n",
        "    def render(self):\n",
        "        \n",
        "        self.race_track = np.zeros((self.MAX_LEN, self.NUM_PLAYERS))\n",
        "        \n",
        "        self.battle_ground = np.zeros((self.MAX_LEN, self.NUM_PLAYERS))\n",
        "        for i in range(self.NUM_PLAYERS):\n",
        "            for j in self.states[0][i*self.NUM_PIECES : (i+1)*self.NUM_PIECES]:\n",
        "                if 0<=j<self.MAX_LEN :\n",
        "                    self.race_track[j, i] = 1\n",
        "            if i != 0:\n",
        "                shift_len = i*self.BLOCK_LEN\n",
        "                self.battle_ground[:shift_len,i] = self.race_track[-shift_len:,i].copy()\n",
        "                self.battle_ground[shift_len:,i] = self.race_track[:-shift_len,i].copy()\n",
        "            else:\n",
        "                self.battle_ground[:,i] = self.race_track[:,i].copy()\n",
        "        self.race_track[self.ALL_SAFE_SPOTS, :] += 0.1\n",
        "        self.battle_ground[self.ALL_SAFE_SPOTS, :] += 0.1\n",
        "        \n",
        "        fig, ax = plt.subplots(1,2)\n",
        "        ax[0].imshow(self.race_track)\n",
        "        ax[1].imshow(self.battle_ground)\n",
        "        plt.show()\n",
        "    \n",
        "    def step(self, action):\n",
        "\n",
        "        move_type, reward = self.move(self.player_turn, action, self.dice_face)    \n",
        "                \n",
        "        self.moves_remaining = min(self.MAX_MOVES - self.moves_made, self.moves_remaining)\n",
        "        if self.moves_remaining <= 0:\n",
        "            \n",
        "            self.player_turn = (self.player_turn + 1) % self.NUM_PLAYERS # next player\n",
        "            self.moves_made = 0\n",
        "            self.moves_remaining = 1\n",
        "        \n",
        "        self.dice_face = np.random.randint(self.DICE_SIDES) + 1\n",
        "        \n",
        "        self.states[1:] = [self.dice_face,\n",
        "                           self.player_turn,\n",
        "                           self.moves_remaining]\n",
        "        \n",
        "        target = np.array(self.target_space)\n",
        "        other_players = [i for i in range(self.NUM_PLAYERS) if i != self.player_turn]\n",
        "        done = (target[self.player_turn] == self.NUM_PIECES) or (np.mean(target[other_players]) == self.NUM_PIECES)\n",
        "        \n",
        "        observation = self.states\n",
        "        \n",
        "        return observation, reward, done, {'move_type':move_type, 'moves_made':self.moves_made}\n",
        "\n",
        "def analytical_agent(positions, dice_face, BLOCK_LEN, BONUS_NUM, SAFE_SPOTS):\n",
        "    \n",
        "    action_space = int(len(positions)/2)\n",
        "\n",
        "    if dice_face == BONUS_NUM:\n",
        "        \n",
        "        q_vals = [0]*action_space\n",
        "        \n",
        "        for n, pos in enumerate(positions[:action_space]):\n",
        "            if pos == -1:\n",
        "                q_vals[n] = 2\n",
        "                return q_vals\n",
        "\n",
        "    to_consider = [positions[i] >= 0 and positions[i] < BLOCK_LEN *2 for i in range(action_space, len(positions))] \n",
        "    back_to_consider = [positions[i] < BLOCK_LEN *2 for i in range(action_space, len(positions))]\n",
        "    \n",
        "    positions_other = [(positions[i] + BLOCK_LEN) % (2*BLOCK_LEN) for i in range(action_space, len(positions))]\n",
        "    potential_positions = [(positions[i] + dice_face) % (2*BLOCK_LEN) for i in range(action_space)]\n",
        "    \n",
        "    safe_spot_shift = [.2 if i in SAFE_SPOTS else 1 for i in potential_positions]\n",
        "    \n",
        "    positions_other_back = [i if i < BLOCK_LEN else i- BLOCK_LEN*2  for i in positions_other ]\n",
        "    back_vals = [[(i - j) for j in positions_other_back] for i in potential_positions]\n",
        "    front_vals = [[(j - i) for j in positions_other] for i in potential_positions]\n",
        "\n",
        "\n",
        "    front_scores = [safe_spot_shift[m] * sum([1/i if i>0 and to_consider[n] \\\n",
        "                         else 0 if not to_consider[n] \\\n",
        "                         else 2 if i==0 else 0 \\\n",
        "                         for n,i in enumerate(j)]) for m,j in enumerate(front_vals)]\n",
        "    back_scores = [sum([-1/i if i>0 and back_to_consider[n] else 0 for n,i in enumerate(j)]) for j in back_vals]\n",
        "    total_score = [front_scores[i] + back_scores[i] for i in range(action_space)]\n",
        "    \n",
        "    total_score = [i+.5 if potential_positions[n] in SAFE_SPOTS else i for n,i in enumerate(total_score)]\n",
        "    \n",
        "    return total_score"
      ],
      "metadata": {
        "id": "gzDvveNoJhM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iu5gPGiiw1n"
      },
      "outputs": [],
      "source": [
        "from Ludo_env import LudoEnv, analytical_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTxrbz1Qiw1o"
      },
      "outputs": [],
      "source": [
        "class Actor_critic(nn.Module):\n",
        "    def __init__(self,learning_rate,state_size, n_actions):\n",
        "        super(Actor_critic, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 128)\n",
        "        self.fc4 = nn.Linear(128, n_actions)\n",
        "        self.optimizer = optim.Adam(self.parameters(),lr=learning_rate)\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, state):\n",
        "#         state = torch.Tensor(state)\n",
        "        layer = F.relu(self.fc1(state))\n",
        "        layer = F.relu(self.fc2(layer))\n",
        "        layer = F.relu(self.fc3(layer))\n",
        "        actions = self.fc4(layer)\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtFVHG0oJyCz"
      },
      "outputs": [],
      "source": [
        "DISCOUNT = 0.9\n",
        "TOT_EPISODES = 100000\n",
        "epsilon = 1\n",
        "EPSILON_DECAY = np.exp(np.log(.5) / (TOT_EPISODES/5))\n",
        "MIN_EPS = 0.01\n",
        "LEARNING_RATE = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7n5_h1diw1r"
      },
      "outputs": [],
      "source": [
        "LR_1 = 0.00001\n",
        "LR_2 = 0.0005\n",
        "DISCOUNT = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3T_MmEpiw1s"
      },
      "outputs": [],
      "source": [
        "# len(positions_adjusted + [dice_face -1, moves_left])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51-VSXcbiw1u"
      },
      "outputs": [],
      "source": [
        "env = LudoEnv(20,2,8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erPRMfnNiw1v"
      },
      "outputs": [],
      "source": [
        "state_size = env.NUM_PIECES * env.NUM_PLAYERS + 2\n",
        "actor = Actor_critic(LR_1,state_size=state_size, n_actions=env.NUM_PIECES)\n",
        "critic = Actor_critic(LR_2,state_size=state_size, n_actions=1)\n",
        "\n",
        "actor.to(device)\n",
        "critic.to(device)\n",
        "\n",
        "\n",
        "log_p = None\n",
        "TOT_EPS = 1000\n",
        "# rew_per_ep = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNI4lV69iw1y",
        "outputId": "103df49e-040c-4f33-fe52-433882233e7a",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "100%|██████████| 1000/1000 [1:02:26<00:00,  3.75s/it]\n"
          ]
        }
      ],
      "source": [
        "for episode in tqdm(range(TOT_EPS)):\n",
        "\n",
        "    previous_turn = 1\n",
        "    observation = env.reset()\n",
        "    \n",
        "    positions_all, dice_face, player_turn, moves_left = observation\n",
        "    \n",
        "    positions_adjusted = positions_all[player_turn*env.NUM_PIECES:] + positions_all[:player_turn*env.NUM_PIECES]\n",
        "\n",
        "    done = False\n",
        "    reward_this_ep = 0\n",
        "\n",
        "    next_player_reward = 0\n",
        "    replay_queue = []\n",
        "    reward_replay = np.array([])\n",
        "\n",
        "    state = positions_adjusted + [dice_face -1, moves_left]\n",
        "    while not done:\n",
        "        \n",
        "        state_ = torch.Tensor(state)\n",
        "        state_ = state_.to(device)\n",
        "        probabilities = F.softmax(actor.forward(state_))\n",
        "        action_p = torch.distributions.Categorical(probabilities)\n",
        "        action = action_p.sample()\n",
        "        log_p = action_p.log_prob(action)\n",
        "        action = action.item()\n",
        "\n",
        "        next_obs, reward, done, info = env.step(action)\n",
        "        p_all_next, dice_face_next, player_turn_next, moves_left_next = next_obs\n",
        "        positions_adjusted_next = p_all_next[player_turn_next*env.NUM_PIECES:] + p_all_next[:player_turn_next*env.NUM_PIECES]\n",
        "        \n",
        "        next_state = positions_adjusted_next + [dice_face_next-1, moves_left_next]\n",
        "        other_player = int(player_turn == 0)\n",
        "        \n",
        "        analytical_state_value = analytical_agent(positions_all,\n",
        "                                      dice_face,\n",
        "                                      env.BLOCK_LEN,\n",
        "                                      env.BONUS_NUM, env.ALL_SAFE_SPOTS)[action]\n",
        "\n",
        "        reward += analytical_state_value\n",
        "        \n",
        "        actor.optimizer.zero_grad()\n",
        "        critic.optimizer.zero_grad()\n",
        "\n",
        "        new_critic = 0\n",
        "        next_state_relevant = torch.Tensor(next_state)\n",
        "        next_state_relevant = next_state_relevant.to(device)\n",
        "        \n",
        "        if not done:\n",
        "            new_critic = critic.forward(next_state_relevant)\n",
        "            \n",
        "\n",
        "        state_relevant = torch.Tensor(state)\n",
        "        state_relevant = state_relevant.to(device)\n",
        "        \n",
        "        critic_value = critic.forward(state_relevant)\n",
        "        reward_relevant = torch.tensor(reward/20)\n",
        "        reward_relevant = reward_relevant.to(device)\n",
        "    \n",
        "        critic_loss = (reward_relevant + DISCOUNT*new_critic - critic_value)**2\n",
        "        actor_loss = -log_p * (reward_relevant + DISCOUNT*new_critic - critic_value)\n",
        "        total_loss = critic_loss + actor_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        actor.optimizer.step()\n",
        "        critic.optimizer.step()\n",
        "        \n",
        "        previous_turn = player_turn\n",
        "\n",
        "        positions_all = p_all_next.copy()\n",
        "        dice_face = dice_face_next\n",
        "        player_turn = player_turn_next\n",
        "        moves_left = moves_left_next\n",
        "        \n",
        "        state = next_state.copy()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeNgw0Ypiw11",
        "outputId": "c5d80ec6-92d9-4b4e-f09b-3899575ba43b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.verbose = False\n",
        "actions_taken = []\n",
        "wins = [0, 0]\n",
        "for episode in range(500):\n",
        "    observation = env.reset()\n",
        "    \n",
        "    positions_all, dice_face, player_turn, moves_left = observation\n",
        "    \n",
        "    positions_adjusted = positions_all[player_turn*env.NUM_PIECES:] + positions_all[:player_turn*env.NUM_PIECES]\n",
        "    \n",
        "    done = False\n",
        "    reward_this_ep = 0\n",
        "    \n",
        "    state = positions_adjusted + [dice_face -1, moves_left]\n",
        "    while not done:\n",
        "        \n",
        "        if player_turn == 0:\n",
        "            \n",
        "            action = np.random.randint(env.NUM_PIECES)\n",
        "        \n",
        "        else:\n",
        "\n",
        "            state_ = torch.Tensor(state)\n",
        "            state_ = state_.to(device)\n",
        "            action_p = torch.distributions.Categorical(F.softmax(actor.forward(state_)))\n",
        "            action = action_p.sample()\n",
        "            log_p = action_p.log_prob(action)\n",
        "            action = action.item()\n",
        "            actions_taken.append(action)\n",
        "        \n",
        "        next_obs, reward, done, _ = env.step(action)\n",
        "#         env.render()\n",
        "        \n",
        "        if done:\n",
        "            wins[player_turn] += 1\n",
        "        \n",
        "        p_all_next, dice_face_next, player_turn_next, moves_left_next = next_obs\n",
        "        positions_adjusted_next = p_all_next[player_turn_next*env.NUM_PIECES:] + p_all_next[:player_turn_next*env.NUM_PIECES]\n",
        "        \n",
        "        next_state = positions_adjusted_next + [dice_face_next-1, moves_left_next]\n",
        "            \n",
        "        positions_all = p_all_next.copy()\n",
        "        dice_face = dice_face_next\n",
        "        player_turn = player_turn_next\n",
        "        moves_left = moves_left_next\n",
        "        \n",
        "        state = next_state.copy()\n",
        "wins[1]/sum(wins)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUNVHvnxm1as",
        "outputId": "ced65938-e1af-4416-d1b8-58028875d8d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter({0: 133347, 6: 141, 1: 111, 7: 94, 3: 77, 4: 76, 2: 75, 5: 57})\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.995290271537118"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from collections import Counter\n",
        "counts = Counter(actions_taken)\n",
        "print(counts)\n",
        "counts[0]/sum(counts.values())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "nkhan6_vivekvij_babarahm_final_project(ActCrit1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}